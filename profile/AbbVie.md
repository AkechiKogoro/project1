#### 1Description of Projects:

a. Why were you brought there initially?

I was brought on board to enhance AbbVie's capability to process and analyze vast amounts of pharmacovigilance data using AWS Big Data solutions, with a focus on improving real-time adverse event monitoring and data-driven decision-making.

b. Current state of the app and production environment upon your arrival

Upon my arrival, the systems were fragmented and the data processing workflows were not optimized for scale and efficiency. The production environment lacked robust automation and continuous integration practices.

c. What the app does

The system is designed to monitor, analyze, and report adverse event data in real-time. It helps in identifying safety signals and potential risks associated with pharmaceutical products.

d. Who was the end user of the app?

The end users were pharmacovigilance experts, regulatory affairs professionals, and data scientists within AbbVie who used the insights generated to make informed decisions regarding drug safety and compliance.

e. Specific implementations

I implemented features like real-time data streaming using Kafka and Spark, automated data workflows with Airflow, and advanced analytics capabilities using NLP and machine learning on unstructured data.

f. Leadership and coding time

I spent approximately 60% of my time leading and strategizing, while the remaining 40% was dedicated to hands-on coding and technical reviews.

g. Challenges faced

Challenges included data silos, inconsistent data quality, and scalability issues. I addressed these by redesigning the data ingestion and processing pipelines, implementing rigorous data validation checks, and using scalable AWS services.

h. Size of the team and roles

The team consisted of around 25 individuals, including data engineers, data scientists, and DevOps specialists.

i. Day-to-day management

I managed the production environment using AWS CloudWatch for monitoring, employed JIRA for task management, and standardized documentation practices using tools like Confluence.

j. Testing environment

The testing environment incorporated automated testing frameworks like ScalaTest and Specs2, alongside Jenkins for continuous integration, ensuring high code quality.

k. Weekly standing meetings

We had weekly scrum meetings involving all team members where we discussed ongoing tasks, blockers, and sprint planning. I facilitated these meetings, ensuring they were focused and action-oriented.

#### 2 Tangible Contributions:

a. Tools and decision-making

I utilized AWS services like S3 and EMR, alongside Snowflake and Databricks, to establish robust data storage and processing architectures that were scalable and secure.

b. Architecture awareness

My decisions often balanced scalability with cost-efficiency, choosing between on-demand processing using AWS Lambda or more persistent solutions like EMR based on the project needs.

c. Layouts awareness

While I primarily focused on backend architectures, I ensured that any user interfaces developed by the team were intuitive and facilitated easy access to analytics for end users.

d. Gathering requirements

Regular meetings with stakeholders were crucial. I ensured these discussions informed the project scope and feature set, translating complex requirements into actionable project milestones.

e. Feature implementation

One key feature was the integration of machine learning models to predict adverse events from unstructured data, which involved both data engineering and science teams.

f. SDLC adherence

Throughout the projects, I ensured that all phases of the SDLC were followed rigorously, from planning and requirements gathering to deployment and maintenance.

g. QA and testing

Testing strategies included unit, integration, and system tests, focusing heavily on data integrity and performance, ensuring the analytics platform remained reliable under different conditions.


h. Codebase acclimation

I quickly familiarized myself with the existing codebase and technologies, leveraging my background in Big Data tools to make impactful enhancements early on.

i. User feedback improvement

By analyzing user feedback and product reviews, I prioritized feature updates that significantly enhanced user satisfaction and system usability.

j. Addressing crash logs and bugs

Regularly reviewed crash logs and backlog items, prioritizing critical bugs that impacted system performance or data accuracy, ensuring robustness.

k. Backlog management

I managed the backlog by categorizing tasks based on priority and impact, aligning development efforts with strategic business goals.

l. Best practices implementation

I advocated for and implemented best coding practices, including OOP principles and clean code standards, to enhance maintainability and scalability.

#### 3 Example of Technical Failure:

- Incident: An outage in the real-time data processing pipeline due to overloaded Kafka brokers.

- Rectification: Quickly implemented a more robust partitioning strategy and scaled up the Kafka cluster resources.

- Key Learning: Reinforced the importance of proactive capacity planning and monitoring, which I now apply in all projects to anticipate and mitigate similar issues.

#### 4 Project Overview:

- Why Hired: To revamp the Big Data infrastructure for pharmacovigilance analytics on AWS, addressing inefficiencies and scaling issues.

- Status at Arrival: Fragmented systems with manual processes.

- Team Collaboration: Worked closely with a cross-functional team to integrate and streamline Big Data workflows, fostering a collaborative and agile work environment.

- Skills/Tech Used: AWS Big Data services, Snowflake, Kafka, Spark, Airflow, and modern CI/CD practices.

- Issues Resolved: Enhanced the real-time monitoring system's reliability and scalability, significantly reducing downtime and data processing delays.

- Deliverables: A robust, scalable real-time adverse event monitoring system and an optimized AWS cloud infrastructure.